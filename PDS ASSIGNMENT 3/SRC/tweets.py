# -*- coding: utf-8 -*-
"""tweets.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1a2bvzNoEPqwAwGy4cvhh9PbcV6xsIreQ
"""

import pandas as pd
import matplotlib.pyplot as plt

data=pd.read_csv(r"/content/Corona_NLP_test.csv",encoding= 'unicode_escape')

#extract required column
OriginalTweet= data["OriginalTweet"]

#write to new csv
data_new=OriginalTweet.to_csv(r"C:\Users\sailo\Downloads\data_new.csv",index=False)

data_new=pd.read_csv(r"C:\Users\sailo\Downloads\data_new.csv",encoding= 'unicode_escape')

data_new

data_list=data_new['OriginalTweet'].values.tolist()

"""A)Convert the text corpus into tokens."""

pip install nltk

import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize

tokens = []
for tweet in data_list:
    tokens += word_tokenize(tweet)
print(tokens)

"""B)Perform stop word removal."""

from nltk.corpus import stopwords

nltk.download('stopwords')

stop_words = set(stopwords.words('english'))

#filter
filtered_tokens = [token for token in tokens if not token.lower() in stop_words]
print(filtered_tokens)

import re
normal_string=re.sub("[^a-zA-Z' ']",' ', " ".join(filtered_tokens))

normal_string

list_new=normal_string.split(" ")
type(list_new)

list_new

#remove_space
delete_empty = [ele for ele in list_new if ele.strip()]
delete_empty

"""C)Count Word frequencies"""

from collections import Counter

# Count the word frequencies
word_freq = dict(Counter(delete_empty))
print(word_freq)

len(word_freq)
type(word_freq)
#(word_freq)

len(word_freq)

word_freq

d = dict((k, v) for k, v in word_freq.items() if v >= 100)
d
len(d)

"""D)Create word clouds."""

pip install wordcloud

all_words = ' '.join(delete_empty)




from wordcloud import WordCloud


wordcloud = WordCloud(max_words=400,width=800, height=500, colormap='Dark2').generate(all_words)
plt.figure(figsize=(12, 8))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

